***
### Exercise 3.1-1
Let f(n) and g(n) be asymptotically nonnegative functions. Using the basic definition of Θ-notation, prove that max(f(n), g(n)) = Θ(f(n)+g(n)).
### `Answer`
By the definition of Θ-notation, we can write  
0 <= c1(f(n) + g(n)) <= max(f(n), g(n)) <= c2(f(n) + g(n))  
To prove this, all the inequalities need to hold true.  
As the max of two function cannot be greater than the sum of two, so c2 = 1.  
Max of two function cannot be less than average of two, so c1 = 1/2.  
And 0 <= c1(f(n) + g(n)) as the functions are nonnegative.  

***
### Exercise 3.1-2
Show that for any real constants a and b, where b > 0,
(n + a)<sup>b</sup> = Θ(n<sup>b</sup>).
### `Answer`
Here (n + a) <= 2n, when |a| <= n and (n + a) >= n/2, when |a| <= n/2.  
So n >= 2a  
So we can write,  
0 <= n/2 <= (n + a) <= 2n  
Now raising to the power b, we get  
0 <= (n/2)<sup>b</sup> <= (n + a)<sup>b</sup> <= (2n)<sup>b</sup>  
0 <= (1/2)<sup>b</sup>n<sup>b</sup> <= (n + a)<sup>b</sup> <= 2<sup>b</sup>n<sup>b</sup>  
Comparing this with 0 <= c1n<sup>b</sup> <= (n + a)<sup>b</sup> <= c2n<sup>b</sup>, we get  
c1 = (1/2)<sup>b</sup> , c2 = 2<sup>b</sup> and n<sub>0</sub> = 2a  
Therefore (n + a)<sup>b</sup> = Θ(n<sup>b</sup>).  

***
### Exercise 3.1-3
Explain why the statement, "The running time of algorithm A is at least O(n<sup>2</sup>)," is meaningless.
### `Answer`
Big-O notation is used to give the upper bound. It shows the worst case running time of an algorithm. If the algorithm A given in above statement is having at least O(n<sup>2</sup>) running time, then worst case running time will be more than O(n<sup>2</sup>) which contradict the statement. So the above statement is meaningless.  

***
### Exercise 3.1-4
Is 2<sup>n+1</sup> = O(2<sup>n</sup>)? Is 2<sup>2n</sup> = O(2<sup>n</sup>)?
### `Answer`
*To prove 2<sup>n+1</sup> = O(2<sup>n</sup>)*  
We need to prove that there exists constants c and n<sub>0</sub> such that  
0 <= 2<sup>n+1</sup> <= c2<sup>n</sup> for all n >= n<sub>0</sub>  
0 <= 2x2<sup>n</sup> <= c2<sup>n</sup>  
dividing by 2<sup>n</sup>, we get  
0 <= 2 <=c  
So c >= 2 and n >= 1  
Hence proved.  
*To prove 2<sup>2n</sup> = O(2<sup>n</sup>)*  
We need to prove that there exists constants c and n<sub>0</sub> such that  
0 <= 2<sup>2n</sup> <= c2<sup>n</sup> for all n >= n<sub>0</sub>  
0 <= 2<sup>n</sup>2<sup>n</sup> <= c2<sup>n</sup>  
dividing by 2<sup>n</sup>, we get  
0 <= 2<sup>n</sup> <= c    
It results into c >= 2<sup>n</sup>  
But this is not possible for large values of n. So it is not possible to prove. So 2<sup>2n</sup> != O(2<sup>n</sup>).

***
### Exercise 3.1-5
Prove Theorem 3.1.  
For any two functions f(n) and g(n), we have f(n) = Θ(g(n)) if and only if f(n) = O(g(n)) and f(n) = Ω(g(n)).
### `Answer`
It is given f(n) = O(g(n)) and f(n) = Ω(g(n)).  
So from the above notations, we can write  
0 <= f(n) <= c<sub>2</sub>g(n) and 0 <= c<sub>1</sub>g(n) <= f(n)  
Combining these two inequalities, we have  
0 <= c<sub>1</sub>g(n) <= f(n) <= c<sub>2</sub>g(n)  
This inequality satisfies the formula of Θ-notation.
Hence proved.

***
### Exercise 3.1-6
Prove that the running time of an algorithm is Θ(g(n)) if and only if its worst-case running time is O(g(n)) and its best-case running time is Ω(g(n)).
### `Answer`
Lets assume that the running time of an algorithm be T(n) = Θ(g(n)).  
then 0 <= c<sub>1</sub>g(n) <= T(n) <= c<sub>2</sub>g(n) for all n >= n<sub>0</sub>  
As 0 <= c<sub>1</sub>g(n) <= T(n) for all n >= n<sub>0</sub>, then T(n) = Ω(g(n)) i.e. T(n) is lower bounded by Ω(g(n)) and is the best case running time.  

As 0 <= T(n) <= c<sub>2</sub>g(n) for all n >= n<sub>0</sub>, then T(n) = O(g(n)) i.e. T(n) is upper bounded by O(g(n)) and is the worst case running time.  
Similarily reverse can be proved.

***
### Exercise 3.1-7
Prove that o(g(n)) ∩ ω(g(n)) is the empty set.
### `Answer`
By definition, o(g(n)) is a set of function f(n) such that 0 <= f(n) < c<sub>1</sub>g(n) for c<sub>1</sub> > 0 and n >= n<sub>0</sub>. ω(g(n)) is a set of function f(n) such that 0 <= c<sub>2</sub>g(n) < f(n) for c<sub>2</sub> > 0 and n >= n<sub>0</sub>.  
So o(g(n)) ∩ ω(g(n)) is a set of function f(n) such that 0 <= c<sub>2</sub>g(n) <= f(n) <= c<sub>1</sub>g(n). For large n this inequality cannot hold true. Therefore no such f(n) exists.

***
### Exercise 3.1-8
We can extend our notation to the case of two parameters n and m that can go to infinity independently at different rates. For a given function g(n, m), we denote by O(g(n, m)) the set of functions  
O(g(n, m)) = {f(n, m): there exist positive constants c, n<sub>0</sub>, and m<sub>0</sub> such that 0 ≤ f(n, m) ≤ cg(n, m)for all n ≥ n<sub>0</sub> and m ≥ m<sub>0</sub>}.  
Give corresponding definitions for Ω(g(n, m)) and Θ(g(n, m)).
### `Answer`
Ω(g(n, m)) = {f(n,m): there exist positive constant c, n<sub>0</sub> and m<sub>0</sub> such that 0 <= cg(n) <= f(n,m) for all n >= n<sub>0</sub> and m >= m<sub>0</sub>}.  
Θ(g(n, m)) = {f(n,m): there exist positive constant c<sub<1</sub>, c<sub>2</sub>, n<sub>0</sub> and m<sub>0</sub> such that 0 <= c<sub>1</sub>g(n) <= f(n,m) <= c<sub>2</sub> for all n >= n<sub>0</sub> and m >= m<sub>0</sub>}.
